{
  "name": "llm-chatter-server",
  "private": true,
  "version": "0.0.8",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "devDependencies": {
    "vite": "^5.3.5"
  },
  "dependencies": {
    "@anthropic-ai/sdk": "^0.32.1",
    "@google/generative-ai": "^0.21.0",
    "@openai/realtime-api-beta": "github:openai/openai-realtime-api-beta",
    "@tensorflow-models/universal-sentence-encoder": "^1.3.3",
    "@tensorflow/tfjs-converter": "^3.21.0",
    "@tensorflow/tfjs-core": "^3.21.0",
    "@tensorflow/tfjs-node": "^4.20.0",
    "axios": "^1.7.7",
    "bcryptjs": "^2.4.3",
    "body-parser": "^1.20.2",
    "chalk": "^5.3.0",
    "cheerio": "^1.0.0-rc.12",
    "cors": "^2.8.5",
    "dotenv": "^16.4.5",
    "express": "^4.19.2",
    "express-rate-limit": "^7.4.1",
    "express-validator": "^7.2.0",
    "fs": "^0.0.1-security",
    "jsonwebtoken": "^9.0.2",
    "langchain": "^0.0.168",
    "openai": "^4.71.1",
    "path": "^0.12.7",
    "strip-ansi": "^7.1.0",
    "ws": "^8.18.0"
  }
}
